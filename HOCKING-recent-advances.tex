% -*- compile-command: "make jss-slides.pdf" -*-
\documentclass{beamer}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\z}{$z = 2, 4, 3, 5, 1$} 

\newcommand{\algo}[1]{\textcolor{#1}{#1}}
\definecolor{PDPA}{HTML}{66C2A5}
\definecolor{CDPA}{HTML}{FC8D62}
\definecolor{GPDPA}{HTML}{4D4D4D}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{Recent advances in supervised optimal changepoint detection}

\author{
  Toby Dylan Hocking --- toby.hocking@nau.edu\\ 
  Northern Arizona University\\
  School of Informatics, Computing, and Cyber Systems\\
  Machine Learning Research Lab --- \url{http://ml.nau.edu}\\
  \includegraphics[height=3.5cm]{photo-atiyeh-whiteboard}
  \includegraphics[height=3.5cm]{2021-03-lab-ski-lunch} \\
  Come to Flagstaff! 
}

\date{}

\maketitle

\section{New algorithms with constraints between adjacent segments}
\begin{frame}
  \frametitle{Changepoint detection algorithms for data over time}
  Neuron spikes, Jewell \emph{et al.}, Biostatistics 2019.

  \includegraphics[width=0.7\textwidth]{intro-neuroscience} 

  Electrocardiograms (heart monitoring), 
  Fotoohinasab \emph{et al.}, 
  Asilomar 2020.

  \includegraphics[width=0.5\textwidth]{intro-ecg} 

\end{frame}

\begin{frame}
  \frametitle{Changepoint detection algorithms for data over space}

  DNA copy number data for cancer diagnosis, Hocking \emph{et
    al.}, Bioinformatics 2014.

  \includegraphics[width=0.8\textwidth]{intro-breakpoints}

  Epigenomic data for understanding the human genome, Hocking 
  \emph{et al.}, Bioinformatics 2017.

  \includegraphics[width=0.8\textwidth]{intro-peaks}

\end{frame}

\begin{frame}
  \frametitle{Optimal changepoint detection problem and algorithms}
  \input{figure-PeakSeg}
  \vskip -1cm    
$$
\min_{\substack{
  \mathbf u\in\RR^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
  }} 
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
$$
  \begin{itemize}
  \item Algorithm inputs $n$ data $z_1, \dots, z_n$ and \# of segments $S$.
  \item Goal is to compute best $S-1$ changepoints
    $t_1 < \cdots < t_{S-1}$ and $S$ segment parameters $u_1,\dots,u_S$.
  \item Hard non-convex optimization problem, na\" ively $O(n^S)$ time.
  \item Auger and Lawrence (1989): $O(Sn^2)$ time algorithm.
  \item Rigaill (2015): $O(n \log n)$ time, unconstrained.
  \item Hocking \emph{et al.} (2020): $O(n \log n)$, directional constraints.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Constrained optimization algorithm speed}
  H, {\it et al.} Journal of Machine Learning Research 21(87):1--40, 2020. 
\vskip -0.5cm
\begin{align*}
\min_{\substack{
  \mathbf u\in\RR^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
}} & \ \
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
\\
      \text{subject to \hskip 0.8cm} &\ \ 
\alert{ u_{s-1} \leq u_s\ \forall s\in\{2,4,\dots\} },
  \nonumber\\
  &\ \ 
\alert{ u_{s-1} \geq u_s\ \forall s\in\{3,5,\dots\} }.
  \nonumber
\end{align*}

\alert{Constraints used to force change up to peak state, then change
  down to background noise state.}

\includegraphics[width=\textwidth]{screenshot-GPDPA-intervals}

\end{frame}

\begin{frame}
  \frametitle{Optimization constraints defined using a graph}
  Runge V \emph{et al.} arXiv:2002.03646.

  \includegraphics[width=0.7\textwidth]{gfpop-up-down}

  \begin{itemize}
  \item Purple Dw/Up nodes represent hidden states.
  \item \#/$\emptyset$ nodes constrain start/end state.
  \item Edges represent possible state transitions.
  \item \texttt{gfpop} R package with C++ code computes optimal
    changepoints for user-defined constraint graphs.
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Isotonic regression (all up changes, constant segments)}
  \includegraphics[width=\textwidth]{gfpop-isotonic}
\end{frame}

\begin{frame}
  \frametitle{All up changes, exponential decaying segments}
  \includegraphics[width=0.5\textwidth]{gfpop-decay}

  Jewell S, Hocking TD, Fearnhead P, Witten D. Fast Nonconvex
  Deconvolution of Calcium Imaging Data. Biostatistics (2019).

  \includegraphics[width=0.7\textwidth]{intro-neuroscience} 

\end{frame}

\begin{frame}
  \frametitle{Many changes up to and down from each spike}
  \includegraphics[width=\textwidth]{gfpop-free-up-down}
\end{frame}

\begin{frame}
  \frametitle{Relevant changes (any direction, large in absolute value)}
  \includegraphics[width=\textwidth]{gfpop-absolute}
\end{frame}

\begin{frame}
  \frametitle{Complex graph for electrocardiogram data}
  \includegraphics[width=0.5\textwidth]{gfpop-ecg-graph}
  \includegraphics[width=\textwidth]{gfpop-ecg-data}

  Fotoohinasab \emph{et al.}, Asilomar conference 2020.

\end{frame}

\section{Computing optimal changepoints subject to label constraints}

\begin{frame}
  \frametitle{What if no models agree with expert labels?}
  Hocking and Rigaill, Pre-print hal-00759129.

  \includegraphics[width=0.8\linewidth]{SegAnnot-motivation}

  \begin{itemize}
  \item Want: one changepoint in each label (red rectangle).
  \item No model is consistent with all three labels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Using expert labels as optimization constraints}
  Hocking and Srivastava, Pre-print arXiv:2006.13967.

  \includegraphics[width=\linewidth]{LOPART-notation}

  \begin{itemize}
  \item Previous OPART model (blue) ignores labels (two errors).
  \item Main idea: add optimization constraints to ensure that there
    is the right number of changepoints predicted in each label.
  \item Proposed LOPART model (black) consistent with labels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Label constraints and up-down constraints}
  Stenberg and Hocking, in progress.

  \includegraphics[width=\linewidth]{FLOPART-example}

  \begin{itemize}
  \item Previous PeakSegOptimal algorithm (bottom) ignores labels (two
    errors).
  \item Proposed FLOPART model (top) consistent with labels, and
    interpretable in terms of peaks and background.
  \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Algorithm with label constraints (LOPART) most accurate}
%   Hocking and Srivastava, Pre-print arXiv:2006.13967.

%   \includegraphics[width=\linewidth]{LOPART-test-roc}
% \end{frame}

\begin{frame}
  \frametitle{Learning a complex graph using labels}
\parbox{0.6\textwidth}{
  \includegraphics[width=\linewidth]{gfpop-ecg-iterations}
} \parbox{0.35\textwidth}{
  \begin{itemize}
  \item  Fotoohinasab \emph{et al.}, 2021.
\item Simple initial graph is iteratively edited (red) to agree with expert
 labeled regions (orange rectangles).
\item Easier for expert to provide labels than graph.
  \end{itemize}
 }
\end{frame}

\section{Learning to predict the number of changepoints}

\begin{frame}
  \frametitle{Max margin interval regression problem similar to SVM}
  Hocking \emph{et al.}, 2013.

  \includegraphics[width=0.8\linewidth]{icml13-hard-margin}
  \begin{itemize}
  \item Train on several data sequences with labels (dots).
  \item Want to compute function between white and black dots.
  \item SVM margin is multi-dimensional (diagonal).
  \item Here margin to maximize is one-dimensional (horizontal).
  \item Learned function predicts number of changepoints/segments.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Test accuracy/AUC in five-fold cross-validation}
  
  Hocking and Killick, useR2017 conference tutorial.

  \includegraphics[width=0.8\linewidth]{supervised-test-metrics}

  Learned linear functions for predicting the number of changepoints
  (IntervalRegressionCV, survreg) are much more accurate than constant
  baseline and unsupervised BIC/SIC.
  
\end{frame}

\begin{frame}
  \frametitle{How to predict }
  
  Drouin \emph{et al.}, Neural Information Processing Systems 2017.

  \includegraphics[width=\linewidth]{mmit-functions}
  \begin{itemize}
  \item Generalization of classical CART regression tree learning algorithm.
  \item Can learning non-linear functions of inputs.
  \item More recently we implemented a similar idea in xgboost,
    Barnwal \emph{et al.}, Pre-print arXiv:2006.04920.
  \end{itemize}
\end{frame}


% ICML'13
% MMIT'17
% ROC opt


\section{Conclusions}

\begin{frame}[fragile]
  \frametitle{Conclusions}

  \begin{itemize}
  \item Optimal detection of $S-1$ changepoints in $n$ data is naively
    a $O(n^S)$ computation.  
  \item Functional pruning method yields algorithms with worst case
    time complexity of $O(n^2)$ (same as classical dynamic
    programming).
  \item Empirically the functional pruning algorithms are much faster,
    $O(n\log n)$.
  \item Only one proof of average time complexity for 1 changepoint
    and the uniform loss function (never used in practice).
  \item Would be interesting to prove $O(n\log n)$ average time complexity in
    other more realistic situations. (square/Poisson loss, $\lambda$) How?
  \item Let's collaborate! toby.hocking@nau.edu
  \end{itemize}
  
\end{frame}

\end{document}
