\documentclass{beamer}
\usepackage{xcolor}
\definecolor{OPART}{HTML}{7F7F7F}
\definecolor{PELT}{HTML}{FF0000}
\definecolor{FPOP}{HTML}{0000FF}
\definecolor{DUST}{HTML}{00BFFF}
\newcommand{\algo}[1]{\textcolor{#1}{#1}}
\usepackage[utf8]{inputenc} % usually not needed (loaded by default)
\usepackage[T1]{fontenc}    % for accent marks.
\usepackage{tabularx}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\z}{$z = 2, 4, 3, 5, 1$} 

\newcommand{\algo}[1]{\textcolor{#1}{#1}}
\definecolor{PDPA}{HTML}{66C2A5}
\definecolor{CDPA}{HTML}{FC8D62}
\definecolor{GPDPA}{HTML}{4D4D4D}

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{
  %Algorithmes efficaces pour la détection de ruptures
  Frontiers in optimal change-point detection algorithms
}

\author{
  Toby Dylan Hocking --- toby.dylan.hocking@usherbrooke.ca\\ 
  Université de Sherbrooke, Département d'Informatique\\
  %Logiciels pour l'Apprentissage, la StatiStique et l'Optimisation --- LASSO Lab \url{http://lassolab.org}\\
  Joint work with Charles Truong, Guillem Rigaill, Vincent Runge\\
  \includegraphics[height=5cm]{2025-01-photo-charles-toby-guillem-vincent.jpg}
}

\date{}

\maketitle

\section{1. Introduction, applications, recent algorithms} 
\begin{frame}
  \frametitle{Changepoint detection algorithms for data over time}
  Neuron spikes, Jewell \emph{et al.}, Biostatistics 2019.

  \includegraphics[width=0.7\textwidth]{intro-neuroscience} 

  Electrocardiograms (heart monitoring), 
  Fotoohinasab \emph{et al.}, 
  Asilomar conference 2020.

  \includegraphics[width=0.5\textwidth]{intro-ecg} 

\end{frame}

\begin{frame}
  \frametitle{Changepoint detection algorithms for data over space}

  DNA copy number data for cancer diagnosis, Hocking \emph{et
    al.}, Bioinformatics 2014.

  \includegraphics[width=0.8\textwidth]{intro-breakpoints}

  ChIP-seq data for annotating active/inactive regions in different cell types, Hocking 
  \emph{et al.}, Bioinformatics 2017.

  \includegraphics[width=0.8\textwidth]{intro-peaks}

\end{frame}

\begin{frame}
  \frametitle{Optimal changepoint detection problem}
  \input{figure-PeakSeg}
  \vskip -1cm    
$$
\min_{\substack{
  \mathbf u\in\RR^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
  }} 
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
$$
\begin{itemize}
  \item Algorithm inputs $n$ data $z_1, \dots, z_n$ and \# of segments $S$.
  \item Loss function $\ell$ measures fit of means $u_s$ to data $z_i$.
  \item Goal is to compute best $S-1$ changepoints
    $t_1 < \cdots < t_{S-1}$ and $S$ segment parameters $u_1,\dots,u_S$.
  \item Non-convex optimization problem, na\" ively $O(n^S)$ time.
  % \item Auger and Lawrence 1989: $O(Sn^2)$ time algorithm.
  % \item Rigaill 2015: $O(n \log n)$ time, change in any direction.
  % \item Hocking \emph{et al.}, 2020: $O(n \log n)$, directional constraints.
  \item Fast approximate heuristic algorithms?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Fast heuristics can yield inaccurate change-points}
  \includegraphics[width=\textwidth]{figure-2d-hmm-sim-noise-many-errors}
  \begin{itemize}
  \item Simulation: angular data, 2 dimensions/features.
  \item Heuristics are approximate optimization algorithms.
  \item Fast: linear $O(n)$ time for $n$ data.
  \item Not guaranteed to compute change-points with best loss.
  \item BINSEG = Binary segmentation (Scott and Knott, 1974).
  \item HMM = Hidden Markov Model (Rabiner, 1989).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Constrained problem, Lasso heuristic}
  \textbf{Constrained} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{
  \sum_{i=1}^{n} \ell( \mu_i,  z_i)
}_{
  \text{Loss (data-fitting)}
}
\text{ subject to }
\underbrace{
  \sum_{i=2}^n I[\mu_{i-1}\neq \mu_i] \leq S.
}_{
  \text{Number of change-points (regularization)}
}
$$
\begin{itemize}
\item Hyper-parameter is number of segments $S$.
\item Indicator function $I[\cdot ]\in\{0,1\}$ is non-convex.
\item Gradient descent yields local min.
\item Analog with best subset regression: hyper-parameter is number of variables (combinatorial search, relax to Lasso).
\item Fused Lasso for change-points (Tibshirani \emph{et al.}, JRSSB 2005).
\item Lasso selects too many variables (false positive change-points).
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Constrained problem, classic dynamic programming solution}
  \textbf{Constrained} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}}\text{ subject to }\underbrace{\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i] \leq S.}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Auger and Lawrence, Algorithms for the optimal identification of
  \textbf{segment neighborhoods}, Bull Math Biol (1989).
\item Optimal recursive updates (dynamic programming algorithm).
\item Let $C_{n,S}$ be best cost up to $n$ data and $S$ segments.
\item Start by computing $C_{1,1}$ to $C_{1,n}$ (cum sum).
\item Then compute $C_{2,2}$ to $C_{2,n}$, $C_{3,3},\dots,C_{3,n}$, etc.
\item Output all optimal models from 1 to $S$ segments.
\item Time complexity $O(S n^2)$ --- faster than naïve $O(n^S)$.
\item Still too slow for large data $n$ and model sizes $S$.
\end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Penalized problem, classic dynamic programming solution}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Hyper-parameter is non-negative penalty $\lambda\geq 0$.
\item Jackson, \emph{et al.}, An algorithm for \textbf{optimal
    partitioning} of data on an interval, IEEE Sig Proc Lett (2005).
\item Optimal recursive updates (dynamic programming algorithm).
\item Let $C_{n}$ be best cost up to $n$ data.
\item Recursively compute $C_{1}, C_2,\dots, C_{n}$.
\item Time complexity $O(n^2)$ --- faster than $O(S n^2)$.
\item User can not directly specify number of segments $S$.
\item Output one optimal change-point model (not all in $1,\dots,S$).
\item Quadratic time is still too slow for large data $n$.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, PELT algorithm}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Recursively compute $C_{1}, C_2,\dots, C_{n}$.
\item Classic DP considers all previous change-points.
\item At time $n$, consider $C_1,\dots,C_{n-1}$.
\item Pruned Exact Linear Time algo: Killick, \emph{et al.}, JASA (2012).
\item At time $n$ consider only a subset of $C_1,\dots,C_{n-1}$.
\item Easy to implement, with only 1 additional line of code!
\item Same output: one optimal change-point model.
\item Time complexity: best $O(n)$, worst $O(n^2)$.
\end{itemize}
\end{frame}

%\section{2. History of optimal change-point algorithms}

\begin{frame}
  \frametitle{Simulated data with constant and linear changes}
  \includegraphics[width=\textwidth]{figure-sim-linear-constant-changes.png}

  \begin{itemize}
  \item Constant changes: always 4 segments (for any data size $n$).
  \item Linear: change every 10 data.
  \end{itemize}
  
  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}


\begin{frame}
  \frametitle{PELT algorithm demonstration}
  \includegraphics[width=\textwidth]{pelt-prune-1.png}

  \begin{itemize}
  \item After each change in data, PELT prunes prior candidates.
  \item More changes---more pruning---faster.
  \end{itemize}
  
  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, PELT algorithm}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{tabular}{cccc}
  \# changes in data & Algorithm & \# candidates per data & Overall time \\
  \hline
  Constant $O(1)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$\\
                     & \algo{PELT} &  $O(n)$ & $O(n^2)$\\
  \hline
  Linear $O(n)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$ \\
   & \algo{PELT} &  \textcolor{PELT}{$O(1)$} & \textcolor{PELT}{$O(n)$} \\
\end{tabular}

\begin{itemize}
\item PELT fast/linear for data with frequent changes.
\item But slow/quadratic for long runs of data without changes.
\item Can we go faster when there are no changes?
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Yes, faster with FPOP algorithm!}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item FPOP algo: Maidstone, \emph{et al.}, Stat. and Comp. (2017).
\item Let $C_n(m)$ be the best cost with mean $m$ at $n$ data.
\item Recursively compute functions $C_{1}(m), C_2(m),\dots, C_{n}(m)$.
\item \textbf{Functional pruning} considers a small subset of candidates.
\item Same output: one optimal change-point model.
\item Same time as PELT in theory: best $O(n)$, worst $O(n^2)$.
\item But much faster in practice!
\end{itemize}
\end{frame}



\begin{frame}
  \frametitle{FPOP algorithm demonstration}
  \includegraphics[width=\textwidth]{fpop-prune-1.png}

  \begin{itemize}
  \item FPOP always prunes, whether or not there are changes in data.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}
 
\begin{frame}
  \frametitle{Penalized problem, FPOP algorithm}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{tabular}{cccc}
  \# changes in data & Algorithm & \# candidates per data & Overall time \\
  \hline
  Constant $O(1)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$\\
                     & \algo{PELT} &  $O(n)$ & $O(n^2)$\\
                     & \algo{FPOP} &  \textcolor{FPOP}{$O(\log n)$} & \textcolor{FPOP}{$O(n\log n)$}\\
  \hline
  Linear $O(n)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$ \\
   & \algo{PELT} &  \textcolor{PELT}{$O(1)$} & \textcolor{PELT}{$O(n)$} \\
   & \algo{FPOP} &  \textcolor{FPOP}{$O(1)$} & \textcolor{FPOP}{$O(n)$} \\
\end{tabular}

\begin{itemize}
\item FPOP always fast in 1d data, no matter how many changes.
\item Worst case $O(n^2)$ only happens in pathological data.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, FPOP algorithm}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item FPOP algo: Maidstone, \emph{et al.}, Stat. and Comp. (2017).
\item Let $C_n(m)$ be the best cost with mean $m$ at $n$ data.
\item Recursively compute functions $C_{1}(m), C_2(m),\dots, C_{n}(m)$.
\item \textbf{Functional pruning} considers a small subset of candidates.
\item Same output: one optimal change-point model.
\item Time complexity best $O(n)$, worst $O(n^2)$.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, FPOP extensions}
Functional pruning can handle:
\begin{itemize}
\item Inequality constraints between segment means (next section).
\item Robust loss functions: Fearnhead and Rigaill,
  %Changepoint detection in the presence of outliers,
  JASA (2019).
\item Auto-regressive models: Romano \emph{et al.},
  %Detecting Abrupt Changes in the Presence of Local Fluctuations and Autocorrelated Noise,
  JASA (2021).
\item Storage on disk: Hocking \emph{et al.}, JSS (2022).
\item Multi-variate time series: Pishchagina \emph{et al.},
  %Geometric-Based Pruning Rules for Change Point Detection in Multiple Independent Time Series,
  Computo (2024).
\item Multi-scale penalties: Liehrmann and Rigaill, JCGS (2025).
\end{itemize}
But:
\begin{itemize}
\item Difficult to implement (100s of lines of C++ code).
\item Not as fast in high dimensional data/models.
\end{itemize}
Is it possible to go faster?
\end{frame}

\begin{frame}
  \frametitle{Yes, faster with DUST algorithm!}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Truong and Runge, An Efficient Algorithm for Exact
  Segmentation of Large Compositional and Categorical Time Series,
  Stat (2024).
\item DUST algo: DUality Sample Test.
\item Combines ideas from PELT and FPOP algos.
\item Solve a Langrange dual problem to prune change-points.
\item Easier to code than FPOP.
\item Same time as PELT/FPOP in theory: best $O(n)$, worst $O(n^2)$.
\item But much faster in practice!
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DUST algorithm demonstration}
  \includegraphics[width=\textwidth]{dust-prune-1.png}

  \begin{itemize}
  \item DUST always prunes, whether or not there are changes in data.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Number of candidates considered}
  \includegraphics[width=\textwidth]{figure-pred-candidates-O.png}

  \begin{itemize}
  \item Data sizes varied from 100 to 10,000,000.
  \item Slope on log-log plot indicates asymptotic time complexity.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Overall computation time}
  \includegraphics[width=\textwidth]{figure-pred-seconds-O.png}

  \begin{itemize}
  \item N= data size computable in time limit of 1 second.
  \item FPOP 10x faster than PELT.
  \item DUST 10x faster than FPOP.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, DUST algorithm}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{tabular}{cccc}
  \# changes in data & Algorithm & \# candidates per data & Overall time \\
  \hline
  Constant $O(1)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$\\
                     & \algo{PELT} &  $O(n)$ & $O(n^2)$\\
                     & \algo{FPOP} &  \textcolor{FPOP}{$O(\log n)$} & \textcolor{FPOP}{$O(n\log n)$}\\
                     & \algo{DUST} &  \textcolor{DUST}{$O(\log n)$} & \textcolor{DUST}{$O(n\log n)$}\\
  \hline
  Linear $O(n)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$ \\
   & \algo{PELT} &  \textcolor{PELT}{$O(1)$} & \textcolor{PELT}{$O(n)$} \\
   & \algo{FPOP} &  \textcolor{FPOP}{$O(1)$} & \textcolor{FPOP}{$O(n)$} \\
   & \algo{DUST} &  \textcolor{DUST}{$O(1)$} & \textcolor{DUST}{$O(n)$} \\
\end{tabular}

\begin{itemize}
\item DUST always fast in 1d data, no matter how many changes.
\item Worst case $O(n^2)$ only happens in pathological data.
\end{itemize}
\end{frame}
 
\begin{frame}
  \frametitle{Computing various solutions using penalized solver}

  How to compute the best model with a given number $P^*$ of change-points? 
  

  \begin{itemize}
  \item Example: $P^*=100$ change-points?
  \item Could run constrained solver to get all models from 0 to $P^*$ change-points.
  \item Required number of DP iterations is linear in number of change-points, $O(P^*)$---slow if $P^*$ large.
  \item Penalized solver returns best change-points for a given
    penalty $\lambda\geq 0$ (but $\lambda$ that yields $P^*$ is
    unknown).
  \item Sequential search: Hocking \emph{et al.}, Journal of Statistical Software 101(10) (2022).
  \item DP iterations logarithmic in number of change-points, $O(\log P^*)$---fast for large $P^*$!
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Penalized (OP) is faster than constrained (SN)}
  
  \includegraphics[width=0.8\textwidth]{figure-OP-faster-than-SN.png}

  \begin{itemize}
  \item Figure: genomic data, $N\approx 10^6$, 
  \item Sequential search repeated runs penalized (OP) solver.
  % \item For large models, penalized (OP) solver much faster
  %   than constrained (SN) solver.
  \item Example: for $N=10^7$, desired change-points $P^*=3000$.
  \item Constrained (SN) solver: 100TB storage, 10 weeks.
  \item Penalized (OP) solver: 100GB storage, 10 hours.
  \end{itemize}
  Hocking \emph{et al.}, Journal of Statistical Software 101(10) (2022).
\end{frame}

\begin{frame}
  \frametitle{Computing ranges of solutions using penalized solver}

  How to compute all models with penalties $\lambda\in[\underline \lambda, \overline \lambda]$?
  \begin{itemize}
  \item Example:  $\lambda\in[0.1, \overline 10.5]$?
  \item CROPS: Change-points for a Range Of PenaltieS.
  \item Haynes, \emph{et al.} Journal of Computational and Graphical Statistics, 26(1), 134-143 (2017).
  \end{itemize}
  
  How to compute all models with number of change-points $P\in[\underline P, \overline P]$?
  \begin{itemize}
  \item Example: all models from 50 to 60 change-points?
  \item CROCS: Change-points for a Range Of ComplexitieS.
  \item Liehrmann \emph{et al.}, BMC Bioinformatics 22(323) (2021).
  \end{itemize}

  If $M$ is the number of models to compute (ex: 11 models from 50 to
  60 change-points), then $O(M + \log \overline P)$ DP iterations---fast for large model sizes $\overline P$.
  
\end{frame}

\section{2. Constraints between adjacent segment means}

\begin{frame}
  \frametitle{Optimization constraints defined using a graph}
  Runge \emph{et al.}, Journal of Statistical Software 2023 (graph figures).

  \includegraphics[width=0.7\textwidth]{gfpop-up-down}

  \begin{itemize}
  \item Purple Dw/Up nodes represent hidden states.
  \item \#/$\emptyset$ nodes constrain start/end state.
  \item Edges represent possible state transitions.
  \item \texttt{gfpop} R package with C++ code computes optimal
    changepoints for user-defined constraint graphs.
  \end{itemize}

\end{frame}


% \begin{frame}
%   \frametitle{Constrained optimization algorithm speed}
%   Hocking {\it et al.}, Journal of Machine Learning Research 2020. 
% \vskip -0.5cm
% \begin{align*}
% \min_{\substack{
%   \mathbf u\in\RR^{S}
% \\
%    0=t_0<t_1<\cdots<t_{S-1}<t_S=n
% }} & \ \
%     \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
% \\
%       \text{subject to \hskip 0.8cm} &\ \ 
% \alert{ u_{s-1} \leq u_s\ \forall s\in\{2,4,\dots\} },
%   \nonumber\\
%   &\ \ 
% \alert{ u_{s-1} \geq u_s\ \forall s\in\{3,5,\dots\} }.
%   \nonumber
% \end{align*}

% \alert{Constraints used to force change up to peak state, then change
%   down to background noise state.}

% \includegraphics[width=\textwidth]{screenshot-GPDPA-intervals}

% \end{frame}

% \begin{frame}
%   \frametitle{Isotonic regression (all up changes, constant segments)}
%   \includegraphics[width=\textwidth]{gfpop-isotonic}
% \end{frame}

\begin{frame}
  \frametitle{All up changes, exponential decaying segments}
  \includegraphics[width=0.5\textwidth]{gfpop-decay}

  Jewell \emph{et al.}, Biostatistics 2019.

  \includegraphics[width=0.7\textwidth]{intro-neuroscience} 

\end{frame}

% \begin{frame}
%   \frametitle{Many changes up to and down from each spike}
%   \includegraphics[width=\textwidth]{gfpop-free-up-down}
% \end{frame}

% \begin{frame}
%   \frametitle{Relevant changes (any direction, large in absolute value)}
%   \includegraphics[width=\textwidth]{gfpop-absolute}
% \end{frame}

\begin{frame}
  \frametitle{Complex graph for electrocardiogram data}
  \includegraphics[width=0.5\textwidth]{gfpop-ecg-graph}
  \includegraphics[width=\textwidth]{gfpop-ecg-data}

  Fotoohinasab \emph{et al.}, Asilomar conference 2020.

\end{frame}

\section{3. Optimal changepoints subject to label constraints}

\begin{frame}
  \frametitle{Learning a complex graph using labels}
\parbox{0.6\textwidth}{
  \includegraphics[width=\linewidth]{gfpop-ecg-iterations}
} \parbox{0.35\textwidth}{
  \begin{itemize}
  \item  Fotoohinasab \emph{et al.}, 2021.
\item Simple initial graph is iteratively edited (red) to agree with expert
 labeled regions (orange rectangles).
\item Easier for expert to provide labels than graph.
  \end{itemize}
 }
\end{frame}

\begin{frame}
  \frametitle{What if no models agree with expert labels?}
  Hocking and Rigaill, Pre-print hal-00759129.

  \includegraphics[width=0.8\linewidth]{SegAnnot-motivation}

  \begin{itemize}
  \item Expert wants: one changepoint in each label (red rectangle).
  \item No model is consistent with all three labels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Using expert labels as optimization constraints}
  Hocking and Srivastava, Computational Statistics 38 (2023).

  \includegraphics[width=\linewidth]{LOPART-notation}

  \begin{itemize}
  \item Previous OPART model (blue) ignores labels (two errors).
  \item Main idea: add optimization constraints to ensure that there
    is the right number of changepoints predicted in each label.
  \item Proposed LOPART model (black) consistent with labels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Label constraints and directional constraints}
   Kaufman \emph{et al.}, Journal of Computational and Graphical Statistics 33(4) (2024).

  \includegraphics[width=\linewidth]{FLOPART-example}

  \begin{itemize}
  \item Previous PeakSegOptimal algorithm (bottom) ignores labels (two
    errors).
  \item Proposed FLOPART model (top) consistent with labels, and
    interpretable in terms of up changes to peaks and down changes to
    background noise.
  \end{itemize}
\end{frame}

\section{4. Learning to predict the number of changepoints}
 
\begin{frame}
  \frametitle{How to predict the number of changes?}

  We assumed that the number of segments $S$ is provided as an input
  parameter to our optimization algorithm.

\begin{align*}
\min_{\substack{
  \mathbf u\in\RR^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
}} & \ \
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
  \nonumber
\end{align*}

In practice $S$ is often unknown --- what value should we use?

\end{frame}

\begin{frame}
  \frametitle{Learning to predict number of changes similar to SVM}
  Hocking \emph{et al.}, Int'l Conference on Machine Learning 2013.

  \includegraphics[width=0.8\linewidth]{icml13-hard-margin}
  \begin{itemize}
  \item Train on several data sequences with labels (dots).
  \item Want to compute function between white and black dots.
  \item SVM margin is multi-dimensional (diagonal).
  \item Here margin to maximize is one-dimensional (horizontal).
  \item Learned function predicts number of changepoints/segments.
  \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Test accuracy/AUC in five-fold cross-validation}
  
%   Hocking and Killick, useR2017 conference tutorial.

%   \includegraphics[width=0.8\linewidth]{supervised-test-metrics}

%   Learned linear functions for predicting the number of changepoints
%   (IntervalRegressionCV, survreg) are much more accurate than constant
%   baseline and unsupervised BIC/SIC.
  
% \end{frame}

\begin{frame}
  \frametitle{Decision tree learns non-linear function of inputs}
  
  Drouin \emph{et al.}, Neural Information Processing Systems 2017.

  \includegraphics[width=\linewidth]{mmit-functions}
  \begin{itemize}
  \item Generalization of classical CART regression tree learning algorithm.
  \item Can learn non-linear functions of inputs.
  \item More recently we implemented a similar idea in xgboost,
    Barnwal \emph{et al.}, Journal of Computational and Graphical Statistics 31(4) (2022).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Is maximizing Area Under the ROC Curve desirable?}
  
  \parbox{2in}{In binary classification the ROC curve is monotonic.} \parbox{2in}{
  \includegraphics[height=1in]{figure-more-than-one-less-auc} 
}

\parbox{2in}{In changepoint detection it can have loops.} \parbox{2in}{
  \includegraphics[height=1in]{figure-more-than-one-more-auc}
}

  \parbox{2in}{We propose instead to minimize the AUM = Area Under
    the Minimum of false positives and false negatives, as a function of prediction threshold.} \parbox{2in}{
  \includegraphics[height=1in]{figure-more-than-one-more-aum}
}

  % \includegraphics[width=0.4\linewidth]{figure-more-than-one-less-auc} 
  % \includegraphics[width=0.4\linewidth]{figure-more-than-one-more-auc}

  % \includegraphics[width=0.7\linewidth]{figure-more-than-one-more-aum}

\end{frame}

\begin{frame}
  \frametitle{AUM gradient descent algorithm optimizes AUC}
  
  Hillman and Hocking, in progress.

  \includegraphics[width=0.49\linewidth]{figure-aum-optimized-iterations-emph}
  \includegraphics[width=0.49\linewidth]{figure-aum-train-pred-only} 

  \begin{itemize}
  \item Initial predictions: minimum label errors.
  \item ROC curves become more regular/monotonic after optimization,
    but label error increases.
  \item Trade-off between AUC and label error optimization that does
    not exist in binary classification.
  \end{itemize}
\end{frame}

\section{Summary and Discussion}

\begin{frame}[fragile]
  \frametitle{Summary and Discussion}

  \begin{itemize}
  \item Optimal changepoint detection in $n$ data is a non-convex
    problem, na\" ively a $O(n^S)$ computation for $S$ segments.
  \item Recent algorithms can compute a globally optimal changepoint
    model much faster, $O(n\log n)$.
  \item Directional constraint graphs specified using domain prior
    knowledge, or learned using expert labels.
  \item Expert labels can also be used as optimization constraints, to
    ensure that predicted changepoints are consistent.
  \item Number of changes can be predicted with new learning
    algorithms, including ROC curve optimization.
  \item Let's collaborate! toby.hocking@nau.edu
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{References}
  \scriptsize
  \begin{itemize}
  \item Auger IE and Lawrence CE. Algorithms for the optimal
    identification of segment neighborhoods. Bull Math Biol 51:39–54
    (1989).
  \item G Rigaill.  A pruned dynamic programming algorithm to recover
    the best segmentations with 1 to kmax change-points. Journal de la
    Société Française de la Statistique, 156(4), 2015. 
  \item \textbf{Hocking TD}, Boeva V, Rigaill G, Schleiermacher G,
    Janoueix-Lerosey I, Delattre O, Richer W, Bourdeaut F, Suguro M,
    Seto M, Bach F, Vert J-P. SegAnnDB: interactive Web-based genomic
    segmentation. Bioinformatics (2014) 30 (11): 1539-1546.
  \item \textbf{Hocking TD}, Goerner-Potvin P, Morin A, Shao X,
    Pastinen T, Bourque G. Optimizing ChIP-seq peak detectors using
    visual labels and supervised machine learning. Bioinformatics
    (2017) 33 (4): 491-499.
  \item Jewell S, \textbf{Hocking TD}, Fearnhead P, Witten D. Fast Nonconvex
    Deconvolution of Calcium Imaging Data. Biostatistics (2019).
  \item Fotoohinasab A, \textbf{Hocking TD}, Afghah F. A
    Graph-Constrained Changepoint Learning Approach for Automatic
    QRS-Complex Detection. Asilomar Conference on Signals, Systems,
    and Computers (2020).
  \item \textbf{Hocking TD}, Rigaill G, Fearnhead P, Bourque G. Constrained
    Dynamic Programming and Supervised Penalty Learning Algorithms for
    Peak Detection in Genomic Data. Journal of Machine Learning
    Research 21(87):1--40, 2020.
  \item Runge V, \textbf{Hocking TD}, Romano G, Afghah F, Fearnhead P, Rigaill
    G. gfpop: an R Package for Univariate Graph-Constrained
    Change-point Detection. Journal of Statistical
    Software 106(6) (2023).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{References}
  \scriptsize
  \begin{itemize}
  \item \textbf{Hocking TD}, Rigaill G, Bach F, Vert J-P. Learning
    sparse penalties for change-point detection using max-margin
    interval regression. International Conference on Machine Learning
    2013.
  \item Drouin A, \textbf{Hocking TD}, Laviolette F. Maximum margin interval
    trees. Neural Information Processing Systems 2017.
  % \item \textbf{Hocking TD} and Killick R. Introduction to optimal
  %   changepoint detection algorithms. useR2017 conference tutorial.
  \item Barnwal A, Cho H, \textbf{Hocking TD}. Survival regression with
    accelerated failure time model in XGBoost. Journal of Computational and Graphical Statistics 31(4) (2022).
  \item Hillman J, \textbf{Hocking TD}. Optimizing ROC Curves with a
    Sort-Based Surrogate Loss Function for Binary Classification and
    Changepoint Detection. Journal of Machine Learning Research 24(70)
    (2023).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{References}
  \scriptsize
  \begin{itemize}
  \item \textbf{Hocking TD}, Rigaill G. SegAnnot: an R package for
    fast segmentation of annotated piecewise constant signals,
    Pre-print hal-00759129.
  \item \textbf{Hocking TD}, Srivastava A. Labeled Optimal
    Partitioning. Computational Statistics 38 (2023).
  \item Fotoohinasab A, \textbf{Hocking TD}, Afghah F. A Greedy Graph
    Search Algorithm Based on Changepoint Analysis for Automatic
    QRS-Complex Detection. Computers in Biology and Medicine 130
    (2021).
  \end{itemize}
\end{frame}

\end{document}
