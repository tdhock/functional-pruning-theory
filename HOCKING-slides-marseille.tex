\documentclass{beamer}
\usepackage{xcolor}
\definecolor{OPART}{HTML}{7F7F7F}
\definecolor{PELT}{HTML}{FF0000}
\definecolor{FPOP}{HTML}{0000FF}
\definecolor{DUST}{HTML}{00BFFF}
\newcommand{\algo}[1]{\textcolor{#1}{#1}}
\usepackage[utf8]{inputenc} % usually not needed (loaded by default)
\usepackage[T1]{fontenc}    % for accent marks.
\usepackage{tabularx}
\usepackage{tikz}
\usepackage[all]{xy}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\z}{$z = 2, 4, 3, 5, 1$} 

% Set transparency of non-highlighted sections in the table of
% contents slide.
\setbeamertemplate{section in toc shaded}[default][100]
\AtBeginSection[]
{
  \setbeamercolor{section in toc}{fg=red} 
  \setbeamercolor{section in toc shaded}{fg=black} 
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\title{
  %Algorithmes efficaces pour la détection de ruptures
  Frontiers in optimal change-point algorithms
}

\author{
  Toby Dylan Hocking --- toby.dylan.hocking@usherbrooke.ca\\ 
  Université de Sherbrooke, Département d'Informatique\\
  %Logiciels pour l'Apprentissage, la StatiStique et l'Optimisation --- LASSO Lab \url{http://lassolab.org}\\
  Collaborators: Charles Truong, Guillem Rigaill, Vincent Runge\\
  \includegraphics[height=5cm]{2025-01-photo-charles-toby-guillem-vincent.jpg}\\
  Thanks to DATAIA for financing 6 month research visit in Paris!
}

\date{}

\maketitle

\begin{frame}
  \frametitle{Why are change-point algorithms so interesting?}

  In statistics, we have data, and we want good parameter estimates:
  \begin{itemize}
  \item Linear model coefficients
  \item Neural networks weights
  \item Change-points
  \end{itemize}

  \begin{tabular}{rccc}
    \hline
    & Linear & Neural Net & Change-point \\
    \hline 
    Problem:  & Convex & Non-convex & Non-convex \\
    Algorithm: & Gradient & Gradient & Dynamic programming \\
    Optimality: & Global & Local & Global \\
    \hline
  \end{tabular}

  \begin{itemize}
  \item Change-point detection is a non-convex problem
  \item with efficient dynamic programming algorithms
  \item that return the globally optimal model parameters!
  \end{itemize}

  This talk: recent computational advances.
\end{frame}

\section{1. Drawbacks of classic algorithms} 
\begin{frame}
  \frametitle{Changepoint detection algorithms for data over time}
  Neuron spikes, Jewell \emph{et al.}, Biostatistics 2019.

  \includegraphics[width=0.7\textwidth]{intro-neuroscience} 

  Electrocardiograms (heart monitoring), 
  Fotoohinasab \emph{et al.}, 
  Asilomar conference 2020.

  \includegraphics[width=0.5\textwidth]{intro-ecg} 

\end{frame}

\begin{frame}
  \frametitle{Changepoint detection algorithms for data over space}

  DNA copy number data for cancer diagnosis, Hocking \emph{et
    al.}, Bioinformatics 2014.

  \includegraphics[width=0.8\textwidth]{intro-breakpoints}

  ChIP-seq data for annotating active/inactive regions in different cell types, Hocking 
  \emph{et al.}, Bioinformatics 2017.

  \includegraphics[width=0.8\textwidth]{intro-peaks}

\end{frame}

\begin{frame}
  \frametitle{Optimal changepoint detection problem}
  \input{figure-PeakSeg}
  \vskip -1cm    
$$
\min_{\substack{
  \mathbf u\in\RR^{S}
\\
   0=t_0<t_1<\cdots<t_{S-1}<t_S=n
  }} 
    \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
$$
\begin{itemize}
  \item Algorithm inputs $n$ data $z_1, \dots, z_n$ and \# of segments $S$.
  \item Loss function $\ell$ measures fit of means $u_s$ to data $z_i$ (Gaussian, Poisson, exponential, $\dots$).
  \item Goal is to compute best $S-1$ changepoints
    $t_1 < \cdots < t_{S-1}$ and $S$ segment parameters $u_1,\dots,u_S$.
  \item Non-convex optimization problem, na\" ively $O(n^S)$ time.
  % \item Auger and Lawrence 1989: $O(Sn^2)$ time algorithm.
  % \item Rigaill 2015: $O(n \log n)$ time, change in any direction.
  % \item Hocking \emph{et al.}, 2020: $O(n \log n)$, directional constraints.
  \item Fast approximate heuristic algorithms?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Fast heuristics can yield inaccurate change-points}
  \includegraphics[width=\textwidth]{figure-2d-hmm-sim-noise-many-errors}
  \begin{itemize}
  \item Simulation: angular data, 2 dimensions/features.
  \item Heuristics are approximate optimization algorithms.
  \item Fast: linear $O(n)$ time for $n$ data.
  \item Not guaranteed to compute change-points with best loss.
  \item BINSEG = Binary segmentation (Scott and Knott, 1974).
  \item HMM = Hidden Markov Model (Rabiner, 1989).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Constrained problem, classic dynamic programming solution}
  \textbf{Constrained} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}}\text{ subject to }\underbrace{\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i] \leq S.}_{\text{Number of change-points (regularization)}}
$$

\includegraphics[width=\textwidth]{SN-demo}

\url{https://tdhock.github.io/SN-demo/}

\end{frame}

\begin{frame}
  \frametitle{Constrained problem, classic dynamic programming solution}
  \textbf{Constrained} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}}\text{ subject to }\underbrace{\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i] \leq S.}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Auger and Lawrence, Algorithms for the optimal identification of
  \textbf{segment neighborhoods}, Bull Math Biol (1989).
\item Optimal recursive updates (dynamic programming algorithm).
\item Let $C_{n,S}$ be best cost up to $n$ data and $S$ segments.
\item Start by computing $C_{1,1}$ to $C_{1,n}$ (cum sum).
\item Then compute $C_{2,2}$ to $C_{2,n}$, $C_{3,3},\dots,C_{3,n}$, etc.
\item Output all optimal models from 1 to $S$ segments.
\item Time complexity $O(S n^2)$ --- faster than naïve $O(n^S)$.
\item Still too slow for large data $n$ and model sizes $S$.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, classic dynamic programming solution}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$

\includegraphics[width=\textwidth]{OP-demo} 

\url{https://tdhock.github.io/OP-demo/}

\end{frame}

\begin{frame}
  \frametitle{Penalized problem, classic dynamic programming solution}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Hyper-parameter is non-negative penalty $\lambda\geq 0$.
\item Jackson, \emph{et al.}, An algorithm for \textbf{optimal
    partitioning} of data on an interval, IEEE Sig Proc Lett (2005).
\item Optimal recursive updates (dynamic programming algorithm).
\item Let $C_{n}$ be best cost up to $n$ data.
\item Recursively compute $C_{1}, C_2,\dots, C_{n}$.
\item Time complexity $O(n^2)$ --- faster than $O(S n^2)$.
\item User can not directly specify number of segments $S$.
\item Output one optimal change-point model (not all in $1,\dots,S$).
\item Quadratic time is still too slow for large data $n$.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, Fused Lasso heuristic}
  \textbf{Penalized relaxed} change-point problem (fused lasso):
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{
  \sum_{i=1}^{n} \ell( \mu_i,  z_i)
}_{
  \text{Loss (data-fitting)}
}
+
\underbrace{
  \lambda  
  \alert{\sum_{i=2}^n |\mu_{i-1}-\mu_{i}|}
}_{
  \text{Absolute differences (regularization)}
}
$$

\includegraphics[width=\textwidth]{FL-demo}

\url{https://tdhock.github.io/FL-demo/}

library(flsa) in R.

\end{frame}

\begin{frame}
  \frametitle{Penalized problem, Fused Lasso heuristic}
  \textbf{Penalized relaxed} change-point problem (fused lasso):
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{
  \sum_{i=1}^{n} \ell( \mu_i,  z_i)
}_{
  \text{Loss (data-fitting)}
}
+
\underbrace{
  \lambda  
  \alert{\sum_{i=2}^n |\mu_{i-1}-\mu_{i}|}
}_{
  \text{Absolute differences (regularization)}
}
$$
\begin{itemize}
\item Hyper-parameter is non-negative penalty $\lambda\geq 0$.
\item Indicator function $I[\cdot ]\in\{0,1\}$ is non-convex.
\item Substitute absolute difference penalty: \alert{$\sum |\cdot|$}.
% \item Analog with best subset regression: hyper-parameter is number of variables (combinatorial search, relax to Lasso).
\item Fused Lasso for change-points (Tibshirani \emph{et al.}, JRSSB 2005).
\item Gradient-based linear time algorithm.
\item Lasso selects too many variables (false positive change-points).
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Summary of classic algorithms}
  Some fast heuristics:
  \begin{itemize}
  \item Binary segmentation (Scott and Knott, 1974).
  \item Hidden Markov Models (Rabiner, 1989).
  \item Fused Lasso (Tibshirani, 2005).
  \end{itemize}
  Quadratic time optimal algorithms:
  \begin{itemize}
  \item Segment neighborhood (Auger and Lawrence, 1989).
  \item Optimal partitioning (Jackson, \emph{et al.}, 2005).
  \end{itemize}
\end{frame}

\section{2. Linear time optimal algorithms with pruning }

\begin{frame}
  \frametitle{Penalized problem, PELT algorithm}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Recursively compute $C_{1}, C_2,\dots, C_{n}$.
\item Classic DP considers all previous change-points.
\item At time $n$, considers all of $C_1,\dots,C_{n-1}$.
\item Pruned Exact Linear Time algo: Killick, \emph{et al.}, JASA (2012).
\item At time $n$, TODO subset of candidates $C_1,\dots,C_{n-1}$.
\item Easy to implement, with only 1 additional line of code!
\item Same output: one optimal change-point model.
\item Time complexity: best $O(n)$, worst $O(n^2)$.
\item library(changepoint) in R.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Simulated data with constant and linear changes}
  \includegraphics[width=\textwidth]{figure-sim-linear-constant-changes.png}

  \begin{itemize}
  \item Constant changes: always 4 segments (for any data size $n$).
  \item Linear: change every 10 data.
  \end{itemize}
  
  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Simulated data with constant and linear changes}
  \includegraphics[width=\textwidth]{figure-sim-linear-constant-changes-model.png}

  \begin{itemize}
  \item Constant changes: always 4 segments (for any data size $n$).
  \item Linear: change every 10 data.
  \end{itemize}
  
  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}


\begin{frame}
  \frametitle{PELT algorithm demonstration}
  \includegraphics[width=\textwidth]{pelt-prune-1.png}

  \begin{itemize}
  \item After each change in data, PELT prunes prior candidates.
  \item More changes---more pruning---faster.
  \end{itemize}
  
  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, PELT algorithm}
%   \textbf{Penalized} optimal change-point problem:
% $$
% \min_{
%   \mathbf \mu\in\RR^{n}
% }
% \underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
% $$
\begin{tabular}{cccc}
  \# changes in data & Algorithm & \# candidates per data & Overall time \\
  \hline
  Constant $O(1)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$\\
                     & \algo{PELT} &  $O(n)$ & $O(n^2)$\\
  \hline
  Linear $O(n)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$ \\
   & \algo{PELT} &  \textcolor{PELT}{$O(1)$} & \textcolor{PELT}{$O(n)$} \\
\end{tabular}

\begin{itemize}
\item PELT fast/linear for data with frequent changes.
\item But slow/quadratic for long runs of data without changes.
\item Can we go faster when there are no changes?
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Yes, faster with FPOP algorithm!}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item FPOP algo: Maidstone, \emph{et al.}, Stat. and Comp. (2017).
\item Let $C_n(m)$ be the best cost with mean $m$ at $n$ data.
\item Recursively compute functions $C_{1}(m), C_2(m),\dots, C_{n}(m)$.
\item \textbf{Functional pruning} considers a small subset of candidates.
\item Same output: one optimal change-point model.
\item Same time as PELT in theory: best $O(n)$, worst $O(n^2)$.
\item But much faster in practice! (never quadratic)
\item library(fpopw) in R.
\end{itemize}
\end{frame}



\begin{frame}
  \frametitle{FPOP algorithm demonstration}
  \includegraphics[width=\textwidth]{fpop-prune-1.png}

  \begin{itemize}
  \item FPOP always prunes, whether or not there are changes in data.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}
 
\begin{frame}
  \frametitle{Penalized problem, FPOP algorithm}
%   \textbf{Penalized} optimal change-point problem:
% $$
% \min_{
%   \mathbf \mu\in\RR^{n}
% }
% \underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
% $$
\begin{tabular}{cccc}
  \# changes in data & Algorithm & \# candidates per data & Overall time \\
  \hline
  Constant $O(1)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$\\
                     & \algo{PELT} &  $O(n)$ & $O(n^2)$\\
                     & \algo{FPOP} &  \textcolor{FPOP}{$O(\log n)$} & \textcolor{FPOP}{$O(n\log n)$}\\
  \hline
  Linear $O(n)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$ \\
   & \algo{PELT} &  \textcolor{PELT}{$O(1)$} & \textcolor{PELT}{$O(n)$} \\
   & \algo{FPOP} &  \textcolor{FPOP}{$O(1)$} & \textcolor{FPOP}{$O(n)$} \\
\end{tabular}

\begin{itemize}
\item FPOP always fast in 1d data, no matter how many changes.
\item Worst case $O(n^2)$ only happens in pathological data.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, FPOP extensions}
Functional pruning can handle:
\begin{itemize}
\item Robust loss functions: Fearnhead and Rigaill,
  %Changepoint detection in the presence of outliers,
  JASA (2019).
\item Auto-regressive models: Romano \emph{et al.},
  %Detecting Abrupt Changes in the Presence of Local Fluctuations and Autocorrelated Noise,
  JASA (2021).
\item Storage on disk: Hocking \emph{et al.}, JSS (2022).
\item Inequality/graph constraints: Runge \emph{et al.}, JSS 2023.
\item Multi-variate time series: Pishchagina \emph{et al.},
  %Geometric-Based Pruning Rules for Change Point Detection in Multiple Independent Time Series,
  Computo (2024).
\item Multi-scale penalties: Liehrmann and Rigaill, JCGS (2025).
\end{itemize}
But:
\begin{itemize}
\item Difficult to implement (100s of lines of C++ code).
\item Especially difficult for high dimensional data/models.
\end{itemize}
Is it possible to go faster?
\end{frame}

\begin{frame}
  \frametitle{Yes, faster with DUST algorithm!}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{itemize}
\item Truong and Runge, An Efficient Algorithm for Exact
  Segmentation of Large Compositional and Categorical Time Series,
  Stat (2024).
\item DUST algo: DUality Simple Test.
\item Combines ideas from PELT and FPOP algos.
\item Solve a Langrange dual problem to prune change-points.
\item Easier to code than FPOP.
\item Same time as FPOP in theory: best $O(n)$, worst $O(n^2)$.
\item But much faster in practice! (constant factors)
\item library(dust) in R.
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DUST algorithm demonstration}
  \includegraphics[width=\textwidth]{dust-prune-1.png}

  \begin{itemize}
  \item DUST always prunes, whether or not there are changes in data.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Number of candidates considered}
  \includegraphics[width=\textwidth]{figure-pred-candidates-O.png}
 
  \begin{itemize}
  \item Slope on log-log plot indicates asymptotic complexity class.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Overall computation time}
  \includegraphics[width=\textwidth]{figure-pred-seconds-O.png}

  \begin{itemize}
  \item N= data size computable in time limit of 1 second.
  \item FPOP 10x faster than PELT.
  \item DUST 10x faster than FPOP.
  \item library(atime) in R for asymptotic analysis.
  \end{itemize}

  \url{https://tdhock.github.io/blog/2025/PELT-vs-fpopw/}
\end{frame}

\begin{frame}
  \frametitle{Penalized problem, DUST algorithm}
  \textbf{Penalized} optimal change-point problem:
$$
\min_{
  \mathbf \mu\in\RR^{n}
}
\underbrace{\sum_{i=1}^{n} \ell( \mu_i,  z_i)}_{\text{Loss (data-fitting)}} + \underbrace{\lambda\sum_{i=2}^n I[\mu_{i-1}\neq \mu_i].}_{\text{Number of change-points (regularization)}}
$$
\begin{tabular}{cccc}
  \# changes in data & Algorithm & \# candidates per data & Overall time \\
  \hline
  Constant $O(1)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$\\
                     & \algo{PELT} &  $O(n)$ & $O(n^2)$\\
                     & \algo{FPOP} &  \textcolor{FPOP}{$O(\log n)$} & \textcolor{FPOP}{$O(n\log n)$}\\
                     & \algo{DUST} &  \textcolor{DUST}{$O(\log n)$} & \textcolor{DUST}{$O(n\log n)$}\\
  \hline
  Linear $O(n)$ & \algo{OPART} &  $O(n)$ & $O(n^2)$ \\
   & \algo{PELT} &  \textcolor{PELT}{$O(1)$} & \textcolor{PELT}{$O(n)$} \\
   & \algo{FPOP} &  \textcolor{FPOP}{$O(1)$} & \textcolor{FPOP}{$O(n)$} \\
   & \algo{DUST} &  \textcolor{DUST}{$O(1)$} & \textcolor{DUST}{$O(n)$} \\
\end{tabular}

\begin{itemize}
\item DUST always fast in 1d data, no matter how many changes.
\item Is it possible to go faster and stay optimal?
\only<2>{\item Conjecture: only by constant factors.}
\end{itemize}
\end{frame}

\section{3. Computing constrained models using penalized solver} 
 
\begin{frame}
  \frametitle{Computing one constrained model using penalized solver}

  How to compute the best model with $P^*$ change-points? 

  \begin{itemize}
  \item Example: want $P^*=100$ change-points.
  \end{itemize}

  Pruned dynamic programming algo: Rigaill, JSFDS (2015).
  \begin{itemize}
  \item Run \alert{constrained} solver to get all model sizes from 0 to 100.
  \item $O(\alert{P^*} n\log n)$ time---fast in $n$.
  \item Slow in $P^*$: we do not want model sizes 0 to 99.
  \item library(fpopw) in R.
  \end{itemize}

  Sequential search algo: Hocking \emph{et al.}, Journal of Statistical Software 101(10) (2022).
  \begin{itemize}
  \item Penalized solver returns best change-points for a given
    penalty $\lambda\geq 0$ (but $\lambda$ that yields $P^*$ is
    unknown).
  \item Run \alert{penalized} solver for an special sequence of $\lambda$ values.
  \item $O(\alert{\log P^*} n\log n)$ time---fast in $n$ and $P^*$! (empirically)
  \item library(PeakSegDisk) in R.
  \end{itemize}
\end{frame}
 
\begin{frame}
  \frametitle{Penalized (OP) is faster than constrained (SN)}
  
  \includegraphics[width=0.8\textwidth]{figure-OP-faster-than-SN.png}

  \begin{itemize}
  \item Figure: genomic data, $N\approx 10^6$, 
  \item Sequential search repeated runs penalized (OP) solver.
  % \item For large models, penalized (OP) solver much faster
  %   than constrained (SN) solver.
  \item Example: for $N=10^7$, desired change-points $P^*=3000$.
  \item Constrained (SN) solver: $\approx$100TB storage, 10 weeks.
  \item Penalized (OP) solver: 100GB storage, 10 hours.
  \end{itemize}
  Hocking \emph{et al.}, Journal of Statistical Software 101(10) (2022).
\end{frame}

\begin{frame}
  \frametitle{Fast computation of large model sizes and ranges}

  How to compute all models with penalties $\lambda\in[\underline \lambda, \overline \lambda]$?
  \begin{itemize}
  \item Example: want all models with $\lambda\in[0.1, \overline 10.5]$.
  \item CROPS: Change-points for a Range Of PenaltieS.
  \item Haynes, \emph{et al.} Journal of Computational and Graphical Statistics, 26(1), 134-143 (2017).
  \item library(changepoint) in R.
  \end{itemize}
  
  How to compute all models with number of change-points $P\in[\underline P, \overline P]$?
  \begin{itemize}
  \item Example: want all models from 50 to 60 change-points.
  \item CROCS: Change-points for a Range Of ComplexitieS.
  \item Liehrmann \emph{et al.}, BMC Bioinformatics 22(323) (2021).
  \item library(CROCS) in R.
  \end{itemize}
  Both exploit the piecewise linear solution path to obtain an efficient algorithm, linear time in the number of models!
  % \begin{itemize}
  % \item Avoid computing smaller 
  % \end{itemize}
  % If $M$ is the number of models to compute (ex: 11 models from 50 to
  % 60 change-points), then $O(M + \log \overline P)$ DP
  % iterations---fast for large model sizes $\overline P$ (no need to
  % compute sizes 0 to 49).
\end{frame}
 
\section{Summary and Discussion}

\begin{frame}[fragile]
  \frametitle{Summary and Discussion}

  \begin{itemize}
  \item Optimal changepoint detection in $n$ data is a non-convex
    problem, na\" ively a $O(n^S)$ computation for $S$ segments.
  \item Pruning algorithms compute a globally optimal change-point
    model much faster, $O(n\log n)$ or even $O(n)$.
  \end{itemize}
  Future work:   
  \begin{itemize}
  \item Fast pruning algorithms for piecewise loss
    functions (L1 etc).
  \item Proving best/worst case time complexity of sequential search
    algorithms?
  \end{itemize}
  Contact: toby.dylan.hocking@usherbrooke.ca

  \centering
\includegraphics[height=3cm]{2025-01-photo-charles-toby-guillem-vincent.jpg}  
\end{frame}

\begin{frame}
  \frametitle{References}
  \scriptsize
  \begin{itemize}
  \item Auger IE and Lawrence CE. Algorithms for the optimal
    identification of segment neighborhoods. Bull Math Biol 51:39–54
    (1989).
  \item G Rigaill.  A pruned dynamic programming algorithm to recover
    the best segmentations with 1 to kmax change-points. Journal de la
    Société Française de la Statistique, 156(4), 2015. 
  \item \textbf{Hocking TD}, Boeva V, Rigaill G, Schleiermacher G,
    Janoueix-Lerosey I, Delattre O, Richer W, Bourdeaut F, Suguro M,
    Seto M, Bach F, Vert J-P. SegAnnDB: interactive Web-based genomic
    segmentation. Bioinformatics (2014) 30 (11): 1539-1546.
  \item \textbf{Hocking TD}, Goerner-Potvin P, Morin A, Shao X,
    Pastinen T, Bourque G. Optimizing ChIP-seq peak detectors using
    visual labels and supervised machine learning. Bioinformatics
    (2017) 33 (4): 491-499.
  \item Jewell S, \textbf{Hocking TD}, Fearnhead P, Witten D. Fast Nonconvex
    Deconvolution of Calcium Imaging Data. Biostatistics (2019).
  \item Fotoohinasab A, \textbf{Hocking TD}, Afghah F. A
    Graph-Constrained Changepoint Learning Approach for Automatic
    QRS-Complex Detection. Asilomar Conference on Signals, Systems,
    and Computers (2020).
  \item \textbf{Hocking TD}, Rigaill G, Fearnhead P, Bourque G. Constrained
    Dynamic Programming and Supervised Penalty Learning Algorithms for
    Peak Detection in Genomic Data. Journal of Machine Learning
    Research 21(87):1--40, 2020.
  \item Runge V, \textbf{Hocking TD}, Romano G, Afghah F, Fearnhead P, Rigaill
    G. gfpop: an R Package for Univariate Graph-Constrained
    Change-point Detection. Journal of Statistical
    Software 106(6) (2023).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{References}
  \scriptsize
  \begin{itemize}
  \item \textbf{Hocking TD}, Rigaill G, Bach F, Vert J-P. Learning
    sparse penalties for change-point detection using max-margin
    interval regression. International Conference on Machine Learning
    2013.
  \item Drouin A, \textbf{Hocking TD}, Laviolette F. Maximum margin interval
    trees. Neural Information Processing Systems 2017.
  % \item \textbf{Hocking TD} and Killick R. Introduction to optimal
  %   changepoint detection algorithms. useR2017 conference tutorial.
  \item Barnwal A, Cho H, \textbf{Hocking TD}. Survival regression with
    accelerated failure time model in XGBoost. Journal of Computational and Graphical Statistics 31(4) (2022).
  \item Hillman J, \textbf{Hocking TD}. Optimizing ROC Curves with a
    Sort-Based Surrogate Loss Function for Binary Classification and
    Changepoint Detection. Journal of Machine Learning Research 24(70)
    (2023).
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{References}
  \scriptsize
  \begin{itemize}
  \item \textbf{Hocking TD}, Rigaill G. SegAnnot: an R package for
    fast segmentation of annotated piecewise constant signals,
    Pre-print hal-00759129.
  \item \textbf{Hocking TD}, Srivastava A. Labeled Optimal
    Partitioning. Computational Statistics 38 (2023).
  \item Fotoohinasab A, \textbf{Hocking TD}, Afghah F. A Greedy Graph
    Search Algorithm Based on Changepoint Analysis for Automatic
    QRS-Complex Detection. Computers in Biology and Medicine 130
    (2021).
  \end{itemize}
\end{frame}

%\section{Constraints between adjacent segment means}

\begin{frame}
  \frametitle{Optimization constraints defined using a graph}
  Runge \emph{et al.}, Journal of Statistical Software 2023.

  \includegraphics[width=0.7\textwidth]{gfpop-up-down}

  \begin{itemize}
  \item Purple Dw/Up nodes represent hidden states.
  \item \#/$\emptyset$ nodes constrain start/end state.
  \item Edges represent possible state transitions.
  \item library(gfpop) in R computes optimal
    change-points for user-defined constraint graphs.
  \end{itemize}

\end{frame}


% \begin{frame}
%   \frametitle{Constrained optimization algorithm speed}
%   Hocking {\it et al.}, Journal of Machine Learning Research 2020. 
% \vskip -0.5cm
% \begin{align*}
% \min_{\substack{
%   \mathbf u\in\RR^{S}
% \\
%    0=t_0<t_1<\cdots<t_{S-1}<t_S=n
% }} & \ \
%     \sum_{s=1}^S\  \sum_{i=t_{s-1}+1}^{t_s} \ell( u_s,  z_i) 
% \\
%       \text{subject to \hskip 0.8cm} &\ \ 
% \alert{ u_{s-1} \leq u_s\ \forall s\in\{2,4,\dots\} },
%   \nonumber\\
%   &\ \ 
% \alert{ u_{s-1} \geq u_s\ \forall s\in\{3,5,\dots\} }.
%   \nonumber
% \end{align*}

% \alert{Constraints used to force change up to peak state, then change
%   down to background noise state.}

% \includegraphics[width=\textwidth]{screenshot-GPDPA-intervals}

% \end{frame}

% \begin{frame}
%   \frametitle{Isotonic regression (all up changes, constant segments)}
%   \includegraphics[width=\textwidth]{gfpop-isotonic}
% \end{frame}

\begin{frame}
  \frametitle{All up changes, exponential decaying segments}
  \includegraphics[width=0.5\textwidth]{gfpop-decay}

  Jewell \emph{et al.}, Biostatistics 2019.

  \includegraphics[width=0.7\textwidth]{intro-neuroscience}

  library(FastLZeroSpikeInference) in R.

\end{frame}

% \begin{frame}
%   \frametitle{Many changes up to and down from each spike}
%   \includegraphics[width=\textwidth]{gfpop-free-up-down}
% \end{frame}

% \begin{frame}
%   \frametitle{Relevant changes (any direction, large in absolute value)}
%   \includegraphics[width=\textwidth]{gfpop-absolute}
% \end{frame}

\begin{frame}
  \frametitle{Complex graph for electrocardiogram data}
  \includegraphics[width=0.5\textwidth]{gfpop-ecg-graph}
  \includegraphics[width=\textwidth]{gfpop-ecg-data}

  Fotoohinasab \emph{et al.}, Asilomar conference 2020.

\end{frame}

%\section{3. Optimal change-points subject to label constraints} 
%\section{3. Optimal change-points subject to label constraints}

\begin{frame}
  \frametitle{Fitting change-points to labels from an expert}
\parbox{0.6\textwidth}{
  \includegraphics[width=\linewidth]{supervised-change-label-error}
} \parbox{0.35\textwidth}{
  \begin{itemize}
  \item Figure: Nguyen and Hocking, arXiv:2505.07413.
  \item fp=false positive (too many changes).
  \item fn=false negative (too few changes).
  \item Try different $\lambda$ values until we find a model with no
    errors. 
  \end{itemize}
 }
\end{frame}

\begin{frame}
  \frametitle{What if no models agree with expert labels?}
  Hocking and Rigaill, Pre-print hal-00759129.

  \includegraphics[width=0.8\linewidth]{SegAnnot-motivation}

  \begin{itemize}
  \item Expert wants: one changepoint in each label (red rectangle).
  \item No model is consistent with all three labels.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Using expert labels as optimization constraints}
  Hocking and Srivastava, Computational Statistics 38 (2023).

  \includegraphics[width=\linewidth]{LOPART-notation}

  \begin{itemize}
  \item Previous OPART model (blue) ignores labels (two errors).
  \item Main idea: add optimization constraints to ensure that there
    is the right number of changepoints predicted in each label.
  \item Proposed LOPART model (black) consistent with labels.
  \item library(LOPART) in R.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Label constraints and directional constraints}
   Kaufman \emph{et al.}, Journal of Computational and Graphical Statistics 33(4) (2024).

  \includegraphics[width=\linewidth]{FLOPART-example}

  \begin{itemize}
  \item Previous PeakSegOptimal algorithm (bottom) ignores labels (two
    errors).
  \item Proposed FLOPART model (top) consistent with labels, and
    interpretable in terms of up changes to peaks and down changes to
    background noise.
  \item library(FLOPART) in R.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Learning a complex graph using labeled regions}
\parbox{0.6\textwidth}{
  \includegraphics[width=\linewidth]{gfpop-ecg-iterations}
} \parbox{0.35\textwidth}{
  \begin{itemize}
  \item  Fotoohinasab \emph{et al.}, 2021.
\item Simple initial graph is iteratively edited (red) to agree with expert
 labeled regions (orange rectangles).
\item Easier for expert to provide labels than graph.
  \end{itemize}
 }
\end{frame}

% \section{4. Predicting the penalty}
%\section{4. Predicting the penalty using labels}
 
\begin{frame}
  \frametitle{How to predict the number of changes?}

  TODO figure.

  In practice the penalty $\lambda$ is unknown --- what value should we use?

\begin{itemize}
\item $z = z_1,\dots,z_n$ is the data sequence to segment.
\item Let $x = \phi(z)$ be a feature vector (fixed, not learned).
\item Learn $f(x) = \log \lambda$, predicted penalty.
\item Goal: minimize error with respect to labeled regions in test set.
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Learning to predict number of changes similar to SVM}
  Hocking \emph{et al.}, Int'l Conference on Machine Learning 2013.

  \includegraphics[width=0.8\linewidth]{icml13-hard-margin}
  \begin{itemize}
  \item Train on several data sequences with labels (dots).
  \item Want to compute function between white and black dots.
  \item SVM margin is multi-dimensional (diagonal).
  \item Here margin to maximize is one-dimensional (horizontal).
  \item L1 regularized linear model, library(penaltyLearning) in R.
  \end{itemize}
\end{frame}

% \begin{frame}
%   \frametitle{Test accuracy/AUC in five-fold cross-validation}
  
%   Hocking and Killick, useR2017 conference tutorial.

%   \includegraphics[width=0.8\linewidth]{supervised-test-metrics}

%   Learned linear functions for predicting the number of changepoints
%   (IntervalRegressionCV, survreg) are much more accurate than constant
%   baseline and unsupervised BIC/SIC.
  
% \end{frame}

\begin{frame}
  \frametitle{Decision tree learns non-linear function of inputs}
  
  Drouin \emph{et al.}, Neural Information Processing Systems 2017.

  \includegraphics[width=\linewidth]{mmit-functions}
  \begin{itemize}
  \item Generalization of classical CART regression tree learning algorithm.
  \item Can learn non-linear functions of inputs.
  \item More recently we implemented a similar idea in xgboost,
    Barnwal \emph{et al.}, Journal of Computational and Graphical Statistics 31(4) (2022).
  \item library(mmit) and library(xgboost) in R.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Penalty prediction using recurrent neural network}
  
  \includegraphics[width=\linewidth]{penalty-pred-RNN}
  \begin{itemize}
  \item Nguyen and Hocking, arXiv:2505.07413.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{AUM gradient descent algorithm optimizes AUC}
  
  Hillman and Hocking, \emph{Journal of Machine Learning Research} 2023.

  \includegraphics[width=0.49\linewidth]{figure-aum-optimized-iterations-emph}
  \includegraphics[width=0.49\linewidth]{figure-aum-train-pred-only} 

  \begin{itemize}
  \item Initial predictions: minimum label errors.
  \item Optimized ROC curves are more regular/monotonic.
  \item Trade-off between AUC and label error optimization.
  \item library(aum) in R.
  \end{itemize}
\end{frame}

\input{figure-sequential-search} 

\end{document}
